# inference_sentiment.py
import re
import random
from collections import Counter

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------
# æ¨¡å‹å®šä¹‰ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
# -------------------------
class SentimentImproved(nn.Module):
    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout=0.3):
        super(SentimentImproved, self).__init__()
        self.hidden_dim = hidden_dim
        self.output_size = output_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(input_size, embedding_dim)
        nn.init.xavier_uniform_(self.embedding.weight)

        self.embed_dropout = nn.Dropout(0.2)

        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                            dropout=dropout, batch_first=True,
                            bidirectional=True)

        self.projection = nn.Linear(embedding_dim, hidden_dim * 2)
        self.attention_weights = nn.Parameter(torch.rand(hidden_dim * 2))

        self.layer_norm1 = nn.LayerNorm(hidden_dim * 2)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)

        self.bn = nn.BatchNorm1d(hidden_dim * 2)
        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)
        nn.init.kaiming_normal_(self.fc1.weight)

        self.dropout = nn.Dropout(dropout)
        self.elu = nn.ELU(inplace=True)

        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_size)
        nn.init.xavier_normal_(self.fc2.weight)

        self.sigmoid = nn.Sigmoid()

    def attention_net(self, lstm_output):
        batch_size, seq_len, hidden_size = lstm_output.size()
        attn_weights = torch.bmm(
            lstm_output,
            self.attention_weights.unsqueeze(0).unsqueeze(2).expand(batch_size, -1, 1)
        ).squeeze(2)
        temp = 1.0
        soft_attn_weights = F.softmax(attn_weights / temp, dim=1).unsqueeze(2)
        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights).squeeze(2)
        return context, soft_attn_weights.squeeze(2)

    def forward(self, x, hidden):
        batch_size = x.size(0)
        x = x.long()
        embeds = self.embedding(x)
        embeds = self.embed_dropout(embeds)
        residual = self.projection(embeds.mean(dim=1))
        lstm_out, hidden = self.lstm(embeds, hidden)
        attn_output, attention_weights = self.attention_net(lstm_out)
        combined = attn_output + residual
        normalized = self.layer_norm1(combined)

        # BatchNorm1d expects (batch, features)
        norm_output = self.bn(normalized)
        fc1_output = self.fc1(norm_output)
        fc1_output = self.dropout(fc1_output)
        fc1_output = self.elu(fc1_output)
        fc1_output = self.layer_norm2(fc1_output)
        fc1_output = self.bn2(fc1_output)
        fc2_output = self.fc2(fc1_output)
        sigmoid_out = self.sigmoid(fc2_output)

        return sigmoid_out, hidden, attention_weights

    def init_hidden(self, batch_size, device):
        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(device)
        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(device)
        nn.init.normal_(h0, mean=0, std=0.01)
        nn.init.normal_(c0, mean=0, std=0.01)
        return (h0, c0)


# -------------------------
# æ–‡æœ¬å¤„ç†å‡½æ•°ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
# -------------------------
def clean_text(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', ' URL ', text)
    text = re.sub(
        r':\)|;\)|:-\)|\(-:|:-D|:D|=-\)|=\)|:-\(|:\(|:-o|:o|:-O|:O|:-0|8-\)|8\)|:-\||:\||:P|:-P|:p|:-p|:b|:-b',
        ' EMOJI ', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' NUM ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# è¿™é‡Œä½¿ç”¨ç®€å•è¯å¹²ï¼ˆåŸè„šæœ¬ä¸­ä½¿ç”¨ PorterStemmerï¼›ä¸ºé¿å…é¢å¤–ä¾èµ–ï¼Œè¿™é‡Œä¿ç•™ä¸€ä¸ªè½»é‡æ›¿ä»£ï¼‰
# å¦‚æœä½ ä»å¸Œæœ›ä½¿ç”¨ NLTK çš„ PorterStemmerï¼Œè¯·è‡ªè¡Œå®‰è£…å¹¶æ›¿æ¢ä¸‹é¢çš„å®ç°ã€‚
try:
    from nltk.stem import PorterStemmer
    _ps = PorterStemmer()
    def stemming(text):
        words = text.split()
        return ' '.join([_ps.stem(w) for w in words])
except Exception:
    # é€€åŒ–ï¼šæ²¡æœ‰ nltk æ—¶ä»…è¿”å›åŸæ–‡ï¼ˆæœ€å¥½æå‰å®‰è£… nltkï¼‰
    def stemming(text):
        return text

# stopwordsï¼ˆå°è¯•ä½¿ç”¨ nltk çš„åœç”¨è¯ï¼›è‹¥ä¸å¯ç”¨åˆ™ä½¿ç”¨ä¸€ä¸ªå°é›†åˆï¼‰
try:
    from nltk.corpus import stopwords as _st
    STOPWORDS = set(_st.words('english'))
except Exception:
    STOPWORDS = set([
        'the','a','an','in','on','and','or','is','are','was','were','be','to','of','for','it','this','that'
    ])

def remove_stopwords(text):
    word_tokens = text.split()
    filtered_text = [word for word in word_tokens if word not in STOPWORDS]
    return ' '.join(filtered_text)

def converts_improved(text, word_int, unknown_token=0):
    clean = clean_text(text)
    clean = remove_stopwords(clean)
    stemmed = stemming(clean)
    text_ints = []
    for word in stemmed.split():
        if word in word_int:
            text_ints.append(word_int[word])
        else:
            text_ints.append(unknown_token)
    return text_ints

def reset_text(text_list, seq_len):
    dataset = np.zeros((len(text_list), seq_len), dtype=np.int64)
    for index, sentence in enumerate(text_list):
        if len(sentence) < seq_len:
            dataset[index, :len(sentence)] = sentence
        else:
            dataset[index, :] = sentence[:seq_len]
    return dataset


# -------------------------
# åŠ è½½æ¨¡å‹ï¼ˆä» checkpoint / saved dictï¼‰
# -------------------------
def load_model_from_checkpoint(checkpoint_path, device=None):
    """
    åŠ è½½ checkpointã€‚checkpoint å¯ä»¥æ˜¯ï¼š
      - ç›´æ¥ä¿å­˜çš„ model.state_dict()ï¼ˆä»…æƒé‡ï¼‰ -> éœ€è¦å¤–éƒ¨æä¾› config/vocab
      - ä¸€ä¸ª dict åŒ…å« keys: 'model_state_dict', 'config', 'vocab'ï¼ˆæ›´ç†æƒ³ï¼‰
    è¿”å› (model, vocab, config)
    """
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    ckpt = torch.load(checkpoint_path, map_location=device)

    vocab = None
    config = None

    # è‹¥ ckpt åŒ…å«å®Œæ•´ä¿¡æ¯
    if isinstance(ckpt, dict) and 'model_state_dict' in ckpt and 'config' in ckpt:
        config = ckpt.get('config')
        vocab = ckpt.get('vocab', None)
        state_dict = ckpt['model_state_dict']
        # å°è¯•ä» config åˆ›å»ºæ¨¡å‹
        if config is None:
            raise ValueError("checkpoint contains model_state_dict but no config")
        model = SentimentImproved(
            input_size=config['input_size'],
            embedding_dim=config['embedding_dim'],
            hidden_dim=config['hidden_dim'],
            output_size=config['output_size'],
            num_layers=config['num_layers'],
            dropout=config.get('dropout', 0.3)
        )
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
        return model, vocab, config

    # è‹¥ ckpt æ˜¯ç›´æ¥ä¿å­˜çš„ state_dictï¼ˆæˆ–æ¨¡å‹ dictï¼‰
    elif isinstance(ckpt, dict):
        # try to guess config keys in ckpt
        if 'config' in ckpt:
            config = ckpt['config']
        if 'vocab' in ckpt:
            vocab = ckpt['vocab']
        # if state_dict is nested under 'model_state_dict'
        if 'model_state_dict' in ckpt:
            state_dict = ckpt['model_state_dict']
        else:
            # assume ckpt itself is state_dict
            state_dict = ckpt

        if config is None:
            raise ValueError("Config needed to reconstruct model is missing in checkpoint. Provide config or a checkpoint saved with 'config'.")
        model = SentimentImproved(
            input_size=config['input_size'],
            embedding_dim=config['embedding_dim'],
            hidden_dim=config['hidden_dim'],
            output_size=config['output_size'],
            num_layers=config['num_layers'],
            dropout=config.get('dropout', 0.3)
        )
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
        return model, vocab, config

    else:
        raise ValueError("Unsupported checkpoint format")


# -------------------------
# å•æ¨¡å‹é¢„æµ‹å‡½æ•°
# -------------------------
def predict_improved(model, text, word_int, device=None, seq_len=400, threshold=0.5):
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    # æ–‡æœ¬è½¬ç´¢å¼•
    text_ints = converts_improved(text, word_int)
    new_text_ints = reset_text([text_ints], seq_len=seq_len)
    text_tensor = torch.from_numpy(new_text_ints).to(device)

    # åˆå§‹åŒ–éšè—çŠ¶æ€
    batch_size = text_tensor.size(0)
    hs = model.init_hidden(batch_size, device)

    with torch.no_grad():
        pred, _, attention_weights = model(text_tensor, hs)

    prob = float(pred.item())
    class_pred = 1 if prob > threshold else 0
    # æ³¨æ„åŠ›æƒé‡ä»…å¯¹å•ä¸ªæ ·æœ¬ï¼ˆbatch size=1ï¼‰è¿”å›
    attention_weights_out = attention_weights.cpu().numpy() if attention_weights is not None else None
    
    # è°ƒæ•´ç½®ä¿¡åº¦è®¡ç®—ï¼Œä½¿å…¶ä¸º 0 åˆ° 1 ä¹‹é—´
    confidence = abs(prob - 0.5) * 2

    return {
        'probability': prob,
        'class': int(class_pred),
        'confidence': float(confidence),
        'attention_weights': attention_weights_out
    }


# -------------------------
# é›†æˆé¢„æµ‹å‡½æ•°ï¼ˆæ¥æ”¶å·²åŠ è½½æ¨¡å‹åˆ—è¡¨ï¼‰
# models_info: list of dicts {'model': model_obj}
# -------------------------
def ensemble_predict(models_info, text, word_int, device=None, seq_len=400, threshold=0.5):
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ–‡æœ¬é¢„å¤„ç†
    text_ints = converts_improved(text, word_int)
    new_text_ints = reset_text([text_ints], seq_len=seq_len)
    text_tensor = torch.from_numpy(new_text_ints).to(device)

    preds = []
    with torch.no_grad():
        for info in models_info:
            model = info['model']
            model.to(device)
            model.eval()
            hs = model.init_hidden(text_tensor.size(0), device)
            pred, _, _ = model(text_tensor, hs)
            preds.append(float(pred.item()))

    avg_pred = float(np.mean(preds))
    class_pred = 1 if avg_pred > threshold else 0
    confidence = abs(avg_pred - 0.5) * 2

    return {
        'probability': avg_pred,
        'class': int(class_pred),
        'confidence': float(confidence),
        'individual_probs': preds
    }


# -------------------------
# æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ï¼ˆå¤šè½®é¢„æµ‹ï¼‰
# -------------------------
if __name__ == "__main__":
    # ä¿®æ”¹ä¸ºä½ ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    CHECKPOINT_PATH = "sentiment_model_optimized.pth"  # æˆ– 'best_sentiment_model.pth'
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print(f"å°è¯•åŠ è½½æ¨¡å‹ï¼š{CHECKPOINT_PATH}...")
    try:
        final_model, word_int, config = load_model_from_checkpoint(CHECKPOINT_PATH, device=device)
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        # åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œä½ éœ€è¦ç¡®ä¿æ¨¡å‹å’Œè¯æ±‡è¡¨æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ã€‚
        print("è¯·ç¡®ä¿æä¾›äº†æ­£ç¡®çš„ CHECKPOINT_PATHï¼Œå¹¶ä¸”è¯¥æ–‡ä»¶ä¸­åŒ…å«æ¨¡å‹æƒé‡ã€è¯æ±‡è¡¨ ('vocab') å’Œé…ç½® ('config')ã€‚")
        raise

    # æ£€æŸ¥ vocab
    if word_int is None:
        raise ValueError("vocab (word_int) not found in checkpoint. è¯·åœ¨ checkpoint ä¸­åŒ…å« 'vocab' æ˜ å°„ã€‚")

    seq_len = config.get('seq_len', 400) if config else 400
    
    # å®šä¹‰å¾…é¢„æµ‹çš„æ–‡æœ¬åˆ—è¡¨
    examples = [
        ("Positive example", "This movie is so amazing. The plot is attractive and I really like it."),
        ("Negative example", "The movie was terrible. I hated every minute of it. The acting was poor."),
        ("Neutral example", "The movie had some good parts and some bad parts. The acting was okay."),
        ("Challenging example", "The movie wasn't bad, but I wouldn't call it good either."),
        ("Sarcastic example", "Wow, what a masterpiece... if you enjoy wasting two hours of your life.")
    ]

    print("\n" + "="*50)
    print("å¼€å§‹å¤šè½®æƒ…æ„Ÿé¢„æµ‹...")
    print("="*50)
    
    # å¾ªç¯è¿›è¡Œé¢„æµ‹
    for name, text in examples:
        print(f"\n--- ç¤ºä¾‹: {name} ---")
        print(f"åŸå§‹æ–‡æœ¬: \"{text}\"")

        # è°ƒç”¨é¢„æµ‹å‡½æ•°
        result = predict_improved(final_model, text, word_int, device=device, seq_len=seq_len, threshold=0.5)

        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        sentiment = "POSITIVE ğŸ‘" if result['class'] == 1 else "NEGATIVE ğŸ‘"
        
        print(f"é¢„æµ‹æƒ…æ„Ÿ: {sentiment}")
        print(f"æ¦‚ç‡ (Positive): {result['probability']:.4f}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f} (0=ä½, 1=é«˜)")
        
        # é¢„å¤„ç†åçš„æ–‡æœ¬ï¼ˆç”¨äºç†è§£æ¨¡å‹çš„è¾“å…¥ï¼‰
        processed_text_ints = converts_improved(text, word_int)
        processed_words = [k for k, v in word_int.items() if v in processed_text_ints and v != 0] # ä»…æ˜¾ç¤ºéUNKè¯æ±‡
        print(f"é¢„å¤„ç†è¯æ±‡æ•°: {len(processed_text_ints)}")
        # print(f"é¢„å¤„ç†è¯æ±‡: {' '.join(processed_words)}") # æ‰“å°æ‰€æœ‰è¯æ±‡å¯èƒ½å¾ˆé•¿

    print("\n" + "="*50)
    print("å¤šè½®é¢„æµ‹å®Œæˆã€‚")